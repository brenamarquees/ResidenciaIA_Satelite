# -*- coding: utf-8 -*-
"""ResidenciaIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SGjH9-sq4ZIRrjK7fGHfYkDC3Og4kaIo

A c√©lula a seguir instala as bibliotecas `python-docx` e `openpyxl`. `python-docx` √© utilizada para interagir com documentos `.docx` do Microsoft Word, enquanto `openpyxl` √© necess√°ria para trabalhar com arquivos `.xlsx` do Microsoft Excel. A instala√ß√£o garante que todas as depend√™ncias estejam prontas para o processamento dos documentos.
"""

!pip install python-docx openpyxl

"""### **Se√ß√£o 1: Configura√ß√£o Inicial e Upload de Arquivos**

Esta se√ß√£o do notebook configura o ambiente e permite que voc√™ carregue os arquivos necess√°rios para o processamento.

#### **1.1 Instala√ß√£o de Bibliotecas Essenciais**

Primeiro, instalamos as bibliotecas `python-docx` e `openpyxl`.

- `python-docx`: Usada para ler e manipular documentos `.docx` (Microsoft Word).
- `openpyxl`: Usada para ler e escrever arquivos `.xlsx` (Microsoft Excel).

#### **1.2 Upload de Arquivos .docx**

Nesta etapa, voc√™ far√° o upload dos seus arquivos `.docx` (especifica√ß√µes t√©cnicas ou documentos Word) que cont√™m as tabelas de pr√©-condi√ß√µes e efeitos. O Colab ir√° solicitar que voc√™ selecione os arquivos do seu computador.

Este bloco de c√≥digo usa a fun√ß√£o `files.upload()` do Google Colab para permitir que voc√™ selecione e carregue arquivos `.docx` do seu computador. Ele imprime uma mensagem instruindo o usu√°rio a selecionar os arquivos e, em seguida, armazena os arquivos carregados na vari√°vel `uploaded`, que √© um dicion√°rio onde as chaves s√£o os nomes dos arquivos e os valores s√£o seus conte√∫dos.
"""

from google.colab import files

print("Selecione um ou mais arquivos .docx (Word)...")
uploaded = files.upload()

"""### **Se√ß√£o 2: Processamento de Documentos Word (DOCX) e Extra√ß√£o de Tabelas**

Esta se√ß√£o cont√©m as classes e fun√ß√µes utilit√°rias que s√£o usadas para extrair informa√ß√µes dos documentos e construir um dicion√°rio de termos, al√©m do script principal que l√™ os arquivos `.docx`, identifica tabelas espec√≠ficas e as salva em formato Excel.

#### **2.1 Defini√ß√£o de Estruturas de Dados e Utilit√°rios**

Este bloco define a classe `Dictionary` para armazenar informa√ß√µes como par√¢metros l√≥gicos, par√¢metros de pot√™ncia, regras de identifica√ß√£o, subsistemas e modelos de tabelas de comportamento. Ele tamb√©m inclui fun√ß√µes para extrair esses dados de PDFs (embora o foco atual seja `.docx`, essas fun√ß√µes demonstram a capacidade de processamento de documentos).

Esta c√©lula define a classe `Dictionary` e fun√ß√µes auxiliares. A classe `Dictionary` √© uma estrutura de dados que organiza informa√ß√µes como par√¢metros l√≥gicos, par√¢metros de pot√™ncia e regras de identifica√ß√£o, que podem ser extra√≠das de documentos. As fun√ß√µes `_norm`, `_capture_keyvals`, `_parse_identification_rules`, `_find_behavior_tables` e `_grab_subsystems` s√£o utilit√°rias para o processamento de texto e extra√ß√£o dessas informa√ß√µes, principalmente de PDFs (embora o foco atual seja .docx, elas mostram a capacidade de processamento de documentos).
"""

from __future__ import annotations
import json, re, unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional

@dataclass
class Dictionary:
    logical_parameters: Dict[str, str]
    power_parameters: Dict[str, str]
    identification_rules: Dict[str, str]
    subsystems: List[str]
    behavior_tables_minimum: List[str]
    behavior_template: Dict[str, List[str]]
    sources: List[str]

    @classmethod
    def from_json(cls, path: str | Path) -> "Dictionary":
        p = Path(path)
        if not p.exists():
            raise FileNotFoundError(f"Dictionary JSON not found: {p}")
        with open(p, "r", encoding="utf-8") as f:
            d = json.load(f)
        return cls(
            logical_parameters=d.get("logical_parameters", {}),
            power_parameters=d.get("power_parameters", {}),
            identification_rules=d.get("identification_rules", {}),
            subsystems=d.get("subsystems", []),
            behavior_tables_minimum=d.get("behavior_tables_minimum", []),
            behavior_template=d.get("behavior_template", {
                "preconditions": ["TM","TC","SW","WK","EX","IN","OM","PW","TH","RF"],
                "effects":       ["TM","OM","WK","IN","PW","TH","RF"],
            }),
            sources=d.get("sources", []),
        )

    def ontology(self) -> Dict[str, str]:
        return {**self.logical_parameters, **self.power_parameters}

    def validators(self) -> Dict[str, re.Pattern]:
        rx = {}
        rx["TM"] = re.compile(r"^TM\d{3}$")
        rx["TC"] = re.compile(r"^TC\d{2}$")
        rx["OM"] = re.compile(r"^OM\d{2}$")
        rx["SW"] = re.compile(r"^SW\d{2}$")
        rx["IN"] = re.compile(r"^IN[A-S]\d{2}$")
        rx["WK"] = re.compile(r"^WK[A-S][A-Z][A-Z]\d{2}$")
        rx["EX"] = re.compile(r"^EX[A-S](?:[A-S])?\d{2}$")
        rx["PW"] = re.compile(r"^PW[A-S](?:[A-Z]{2})?\d{2}$")
        rx["TH"] = re.compile(r"^TH[A-S][A-Z]{2}\d{2}$")
        rx["RF"] = re.compile(r"^RF[A-S][A-Z]{2}\d{2}$")
        return rx

    def validate_code(self, token: str) -> Optional[str]:
        for k, pat in self.validators().items():
            if pat.match(str(token)):
                return k
        return None

    def header_type(self, header: str) -> Optional[str]:
        t = self.validate_code(str(header))
        if t:
            return t
        H = str(header).upper()
        for t in {"TM","TC","OM","SW","WK","EX","IN","PW","TH","RF"}:
            if re.search(rf"\b{t}\b", H):
                return t
        return None

# ---------- Fun√ß√µes para extrair dicion√°rio a partir de PDFs ----------

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", s)
    return s.replace("‚Äì","-").replace("‚Äî","-").strip()

def extract_dictionary_from_pdfs(pdf_paths: List[str | Path]) -> Dictionary:
    import fitz, re

    def _capture_keyvals(lines, valid_keys=None):
        out = {}
        for ln in lines:
            ln = _norm(ln)
            m = re.match(r"^([A-Z]{2})\s+([A-Za-z][A-Za-z /&()\-\]+)$", ln)
            if m:
                k, v = m.group(1).strip(), m.group(2).strip()
                if (not valid_keys) or (k in valid_keys):
                    out[k] = v
        return out

    def _parse_identification_rules(lines):
        rules = {}
        for ln in lines:
            ln = _norm(ln)
            if ("(" in ln and ")" in ln) and (" - " in ln or " ‚Äì " in ln or "‚Äî" in ln):
                try:
                    lhs, rhs = ln.split("(", 1)
                    key = _norm(lhs.split("-")[0]).split("‚Äì")[0].split("‚Äî")[0].strip()
                    val = rhs.rsplit(")",1)[0].strip()
                    key = key.split()[0]
                    if len(key) in (2,3):
                        rules[key] = val
                except ValueError:
                    pass
        return rules

    def _find_behavior_tables(lines):
        tables, seen = [], set()
        for ln in lines:
            ln = _norm(ln)
            if "Telecommands" in ln and "External Events" in ln and "Switch" in ln:
                tables.append(ln)
            if "Working States" in ln and "Internal Registers" in ln:
                tables.append(ln)
            if any(k in ln for k in ["Power Figures","Telemetry Values","Operating Modes"]):
                if " x " in ln or "√ó" in ln or "Telemetry" in ln:
                    tables.append(ln)
        cleaned = []
        for t in tables:
            import re as _re
            t = _re.sub(r"\s+"," ",t)
            if t not in seen:
                seen.add(t); cleaned.append(t)
        return cleaned

    KNOWN_SSS = {"STRU","TCSS","AOCS","EPSS","TTCS","OBDH","SYSC","PAN","IRS","PIT","MWT","DCS","SEM","MUX","DDR","WFI"}

    def _grab_subsystems(text):
        subs=set()
        import re as _re
        for m in _re.finditer(r"\b([A-Z]{3,4})\b", text):
            tok = m.group(1)
            if tok in KNOWN_SSS: subs.add(tok)
        return sorted(subs)

    result = {
        "logical_parameters": {},
        "power_parameters": {},
        "identification_rules": {},
        "subsystems": [],
        "behavior_tables_minimum": [],
        "behavior_template": {
            "preconditions": ["TM","TC","SW","WK","EX","IN","OM","PW","TH","RF"],
            "effects":       ["TM","OM","WK","IN","PW","TH","RF"]
        },
        "sources": []
    }

    for path in pdf_paths:
        p = Path(path)
        if not p.exists():
            print(f"‚ö†Ô∏è PDF n√£o encontrado: {path}")
            continue
        doc = fitz.open(path)
        all_text = []
        for pg in doc:
            txt = pg.get_text("text")
            all_text.append(txt)
            lines = [ln for ln in txt.splitlines() if ln.strip()]
            if "Logical Parameters" in txt or "logical parameters" in txt:
                result["logical_parameters"].update(
                    _capture_keyvals(lines, {"TM","TC","OM","SW","WK","EX","IN"})
                )
            if "Power Parameters" in txt or "power parameters" in txt:
                result["power_parameters"].update(
                    _capture_keyvals(lines, {"PW","TH","RF"})
                )
            if "Identification Codes" in txt or "labeling rule" in txt:
                result["identification_rules"].update(_parse_identification_rules(lines))
            if "5.1.3" in txt or "Model Behavior" in txt:
                bt = _find_behavior_tables(lines)
                if bt: result["behavior_tables_minimum"] = bt
        subs = _grab_subsystems("\n".join(all_text))
        if subs:
            result["subsystems"] = subs
        result["sources"].append(p.name)

    return Dictionary(
        logical_parameters=result["logical_parameters"],
        power_parameters=result["power_parameters"],
        identification_rules=result["identification_rules"],
        subsystems=result["subsystems"],
        behavior_tables_minimum=result["behavior_tables_minimum"],
        behavior_template=result["behavior_template"],
        sources=result["sources"],
    )

"""#### **2.2 Extra√ß√£o de Tabelas de DOCX para Excel**

Este script itera sobre os arquivos `.docx` que voc√™ carregou. Para cada documento:

1.  Ele abre o documento e busca por tabelas.
2.  Converte cada tabela em um DataFrame do Pandas.
3.  **Filtra as tabelas**: Apenas tabelas que cont√™m as palavras "Preconditions" e "Effects" em seu cabe√ßalho s√£o consideradas relevantes.
4.  As tabelas relevantes s√£o agrupadas e salvas em um novo arquivo `.xlsx` (Excel), onde cada tabela se torna uma aba separada.
5.  O arquivo Excel resultante √© automaticamente baixado para o seu computador.

O c√≥digo abaixo processa os arquivos `.docx` carregados, extraindo tabelas espec√≠ficas e salvando-as em arquivos Excel. Primeiramente, ele itera sobre cada arquivo `.docx`. Para cada documento, ele abre o arquivo, busca por todas as tabelas e verifica se o cabe√ßalho da tabela cont√©m as palavras "Preconditions" e "Effects" (ignorando mai√∫sculas/min√∫sculas). Somente as tabelas que satisfazem essa condi√ß√£o s√£o convertidas em DataFrames do Pandas e adicionadas a uma lista. Finalmente, todas as tabelas relevantes de um documento s√£o salvas em um √∫nico arquivo `.xlsx`, com cada tabela em uma aba separada, e o arquivo Excel √© automaticamente baixado.
"""

import re
import os
import pandas as pd
from docx import Document
from google.colab import files

def has_preconditions_effects_header(header_row):
    """
    Verifica se o cabe√ßalho da tabela cont√©m 'Preconditions' e 'Effects'.
    """
    text = " ".join((c or "").strip().lower() for c in header_row)
    return ("preconditions" in text) and ("effects" in text)

for filename in uploaded.keys():
    if not filename.lower().endswith(".docx"):
        print(f"‚ö†Ô∏è Pulando '{filename}' (n√£o √© .docx).")
        continue

    print(f"\nüìÑ Processando documento: {filename}")
    doc = Document(filename)

    if not doc.tables:
        print(f"‚ö†Ô∏è Nenhuma tabela encontrada em {filename}.")
        continue

    tables_to_save = []   # lista com dataframes
    sheet_names = []      # nomes das abas

    for t_idx, table in enumerate(doc.tables):
        # Converte tabela do Word para lista de listas
        rows = []
        for row in table.rows:
            rows.append([cell.text.strip() for cell in row.cells])

        if not rows:
            continue

        # Garante mesmo n¬∫ de colunas em todas as linhas
        max_cols = max(len(r) for r in rows)
        rows = [r + [''] * (max_cols - len(r)) for r in rows]

        header = rows[0]

        # S√≥ pega tabelas cujo cabe√ßalho tenha Preconditions + Effects
        if not has_preconditions_effects_header(header):
            print(f"  - Tabela {t_idx+1}: ignorada (n√£o √© Preconditions/Effects).")
            continue

        print(f"  ‚úÖ Tabela {t_idx+1}: adicionada ao Excel.")

        df = pd.DataFrame(rows)
        tables_to_save.append(df)
        sheet_names.append(f"Tabela_{t_idx+1}")

    if not tables_to_save:
        print(f"‚ö†Ô∏è Nenhuma tabela v√°lida encontrada em {filename}.")
        continue

    # Nome do arquivo Excel
    out_excel = os.path.splitext(filename)[0] + "_tabelas.xlsx"

    with pd.ExcelWriter(out_excel, engine="openpyxl") as writer:
        for df, sh in zip(tables_to_save, sheet_names):
            df.to_excel(writer, sheet_name=sh, header=False, index=False)

    print(f"‚úÖ Excel gerado contendo uma aba por tabela encontrada: {out_excel}")
    files.download(out_excel)

# ------------------------------------------
# EXTRAIR SIGLAS DAS TABELAS E GERAR DICION√ÅRIO
# ------------------------------------------

# print("\nüîé Extraindo siglas das tabelas para gerar dicion√°rio...")

# instanciar o dicion√°rio com regex prontas
# dictionary = Dictionary(
  #  logical_parameters={},
  #  power_parameters={},
  #  identification_rules={},
  #  subsystems=[],
  #  behavior_tables_minimum=[],
  #  behavior_template={
  #      "preconditions": ["TM","TC","SW","WK","EX","IN","OM","PW","TH","RF"],
  #      "effects":       ["TM","OM","WK","IN","PW","TH","RF"],
  #  },
  #  sources=[]
# )

# Regex geral para capturar potenciais c√≥digos, tipo TM123, PWAA12...
# token_re = re.compile(r"[A-Z]{2}[A-Z]?[A-Z]?\d{2,3}")

# siglas_encontradas = {}

# for df in tables_to_save:
  #  for col in df.columns:
   #     for valor in df[col]:
    #        tokens = token_re.findall(str(valor))
     #       for t in tokens:
      #          categoria = dictionary.validate_code(t)
       #         if categoria:
        #            siglas_encontradas.setdefault(categoria, set()).add(t)

# converter sets para listas
# siglas_encontradas = {k: sorted(list(v)) for k, v in siglas_encontradas.items()}

# salvar como JSON
# output_dict = filename.replace(".docx", "_dic_siglas.json")
# with open(output_dict, "w", encoding="utf-8") as f:
  #  json.dump(siglas_encontradas, f, indent=4, ensure_ascii=False)

# print(f"‚úÖ Dicion√°rio gerado: {output_dict}")
# files.download(output_dict)

"""### **Se√ß√£o 3: Desenvolvimento do Modelo de Machine Learning e Gera√ß√£o de C√≥digo**

Esta se√ß√£o implementa um pipeline completo de Machine Learning, com o objetivo de aprender as regras de "pr√©-condi√ß√£o" e "efeito" das tabelas extra√≠das e, em seguida, gerar c√≥digo Python e C++ que encapsule essas regras. Isso permite automatizar a l√≥gica de decis√£o baseada nos dados da especifica√ß√£o.

Esta c√©lula inicia o processo de Machine Learning para a primeira abordagem. Primeiramente, instala as bibliotecas necess√°rias (pandas, scikit-learn, openpyxl, matplotlib). Em seguida, solicita o upload do arquivo Excel gerado na etapa anterior, que cont√©m as tabelas de pr√©-condi√ß√µes e efeitos. O c√≥digo l√™ todas as abas desse arquivo Excel, unifica os dados em um √∫nico DataFrame, identifica as colunas de pr√©-condi√ß√µes e combina todas as colunas de "Effects" em uma √∫nica coluna chamada `Effect_Final`. Trata valores ausentes, codifica as vari√°veis categ√≥ricas usando OneHotEncoder e divide os dados em conjuntos de treino e teste para preparar o treinamento do modelo de √°rvore de decis√£o.
"""

# ============================================
# 1. Instalar depend√™ncias (se necess√°rio)
# ============================================
!pip install -q pandas scikit-learn openpyxl matplotlib

# ============================================
# 2. Imports
# ============================================
import pandas as pd
import numpy as np
import sys # Importar o m√≥dulo sys

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier, export_text, _tree
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

import matplotlib.pyplot as plt

from google.colab import files


# ============================================
# 3. Upload do arquivo Excel
# ============================================
print("Fa√ßa upload do arquivo .xlsx com as tabelas:")
uploaded = files.upload()

excel_filename = list(uploaded.keys())[0]
print(f"Arquivo carregado: {excel_filename}")


# ============================================
# 4. Ler TODAS as abas de tabela e unificar
# ============================================
xlsx = pd.ExcelFile(excel_filename)

all_dfs = []
for sheet_name in xlsx.sheet_names:
    # Se quiser filtrar s√≥ abas que come√ßam com "Tabela", descomente:
    # if not sheet_name.lower().startswith("tabela"):
    #     continue

    df = pd.read_excel(xlsx, sheet_name=sheet_name)
    df["Tabela_ID"] = sheet_name
    all_dfs.append(df)

full_df = pd.concat(all_dfs, ignore_index=True)
print("Formato do dataset unificado:", full_df.shape)
print("Colunas encontradas:")
print(full_df.columns)


# ============================================
# 5. Identificar colunas de pr√©-condi√ß√µes
#    e criar UMA coluna de efeito combinando Effects*
# ============================================
precondition_cols = [c for c in full_df.columns if "preconditions" in c.lower()]
# suas colunas: ['Preconditions','Preconditions.1','Preconditions.2','Preconditions.3']

print("\nColunas de pr√©-condi√ß√µes detectadas:", precondition_cols)

# Todas as colunas que come√ßam com "Effects"
effect_source_cols = [c for c in full_df.columns if c.lower().startswith("effects")]
print("Colunas de efeitos brutas:", effect_source_cols)

if len(precondition_cols) == 0 or len(effect_source_cols) == 0:
    raise ValueError("N√£o foi poss√≠vel detectar colunas de pr√©-condi√ß√£o e/ou efeito. Verifique os nomes.")

# --- combinar todos os Effects* em uma √∫nica coluna Effect_Final ---
effects_df = full_df[effect_source_cols].copy()

# Tratar '-' e strings vazias como "sem efeito"
effects_df = effects_df.replace("-", np.nan)
effects_df = effects_df.replace("", np.nan)

# Converter tudo para string, mas manter NaN
for c in effects_df.columns:
    effects_df[c] = effects_df[c].astype("string")

def combine_effects(row):
    # pega s√≥ valores n√£o nulos
    vals = [v for v in row if pd.notna(v)]
    if not vals:
        return np.nan
    # se tiver mais de um efeito na linha, junta com '+'
    return "+".join(vals)

full_df["Effect_Final"] = effects_df.apply(combine_effects, axis=1)

print("\nExemplos de Effect_Final:")
print(full_df[["Effect_Final"]].head(15))


# ============================================
# 6. Montar X (features) e y (label) + limpeza
# ============================================
feature_cols = precondition_cols + ["Tabela_ID"]
effect_col = "Effect_Final"

data = full_df[feature_cols + [effect_col]].copy()

# Remove linhas sem efeito final
data = data.dropna(subset=[effect_col])

print("\nTamanho do dataset ap√≥s juntar efeitos e remover NaN:", data.shape)

# Preencher NaN nas pr√©-condi√ß√µes com string "MISSING" (evita problema no OneHotEncoder)
for col in feature_cols:
    data[col] = data[col].astype("string").fillna("MISSING")

X = data[feature_cols]
y = data[effect_col].astype("string")

print("\nExemplo de X:")
print(X.head())
print("\nExemplo de y:")
print(y.head())

X = data[feature_cols]
y = data[effect_col].astype("string")

print("\nDistribui√ß√£o das classes (efeitos):")
print(y.value_counts())


# ============================================
# 7. Codifica√ß√£o categ√≥rica + divis√£o treino/teste
# ============================================
categorical_features = feature_cols

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_encoded,
    test_size=0.2,
    random_state=42,
    shuffle=True
)

print("\nTamanhos:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)


# ============================================
# 8. Montar pipeline de IA (√Årvore de Decis√£o)
# ============================================
clf = DecisionTreeClassifier(
    random_state=42,
    max_depth=None,
    min_samples_leaf=1
)

model = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf)
])

model.fit(X_train, y_train)


# ============================================
# 9. Avalia√ß√£o do modelo
# ============================================
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\nAcur√°cia no conjunto de teste: {acc:.4f}")

cm = confusion_matrix(y_test, y_pred)

# √≠ndices das classes que realmente apareceram em y_test ou y_pred
labels_used = np.unique(np.concatenate([y_test, y_pred]))
display_labels = label_encoder.classes_[labels_used]

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=display_labels
)

fig, ax = plt.subplots(figsize=(8, 8))
disp.plot(ax=ax, xticks_rotation="vertical")
plt.title("Matriz de confus√£o - Efeitos (Effect_Final)")
plt.tight_layout()
plt.show()


# ============================================
# 10. Visualizar a √°rvore de decis√£o (regras)
# ============================================
preprocessor.fit(X_train)
ohe = preprocessor.named_transformers_["cat"]
ohe_feature_names = ohe.get_feature_names_out(categorical_features)

tree = model.named_steps["clf"]

tree_rules_text = export_text(tree, feature_names=list(ohe_feature_names))
print("\n===== REGRAS DA √ÅRVORE DE DECIS√ÉO (FORMATO TEXTO) =====")
print(tree_rules_text)


# ============================================
# 11. Fun√ß√£o de infer√™ncia em Python usando o modelo treinado
# ============================================
def prever_efeito(preconditions_dict):
    """
    preconditions_dict: dicion√°rio com os mesmos nomes de colunas de X:
      - 'Preconditions'
      - 'Preconditions.1'
      - 'Preconditions.2'
      - 'Preconditions.3'
      - 'Tabela_ID'

    Exemplo:
      {
        'Preconditions': 'TCM01',
        'Preconditions.1': 'ON',
        'Preconditions.2': 'OFF',
        'Preconditions.3': 'MISSING',
        'Tabela_ID': 'Tabela_17'
      }
    """
    row = {}
    for col in feature_cols:
        val = preconditions_dict.get(col, "MISSING")
        row[col] = "MISSING" if (val is None or str(val) == "nan") else str(val)

    row_df = pd.DataFrame([row])
    y_pred = model.predict(row_df)[0]
    efeito = label_encoder.inverse_transform([y_pred])[0]
    return efeito

print("\n===== EXEMPLO DE PREDI√á√ÉO =====")
exemplo = {}
for col in feature_cols:
    exemplo[col] = X.iloc[0][col]

print("Entrada de exemplo:", exemplo)
print("Efeito previsto:", prever_efeito(exemplo))


# ============================================
# 12. Gera√ß√£o de c√≥digo Python em if/else a partir da √°rvore
#      (em cima das features one-hot)
# ============================================
def gerar_codigo_if_else_da_arvore(tree, feature_names, label_encoder, nome_funcao="decidir_efeito_one_hot"):
    """
    Gera c√≥digo Python com if/else em cima das features j√° one-hot-encoded (bin√°rias).
    Usa a estrutura interna da √°rvore do sklearn.
    """

    tree_ = tree.tree_
    classes = label_encoder.classes_

    def recurse(node, depth):
        indent = "    " * depth

        # AGORA SIM: condi√ß√£o correta para n√≥ folha
        if tree_.feature[node] == _tree.TREE_UNDEFINED:
            class_id = np.argmax(tree_.value[node][0])
            class_label = classes[class_id]
            return f"{indent}return '{class_label}'\n"
        else:
            feature = feature_names[tree_.feature[node]]
            threshold = tree_.threshold[node]

            # como √© OneHot, geralmente threshold ~ 0.5
            cond = f"{feature} <= {threshold:.5f}"

            code = ""
            code += f"{indent}if {cond}:\n"
            code += recurse(tree_.children_left[node], depth + 1)
            code += f"{indent}else:\n"
            code += recurse(tree_.children_right[node], depth + 1)
            return code

    header = f"def {nome_funcao}({', '.join(feature_names)}):\n"
    body = recurse(0, 1)
    return header + body

codigo_if_else = gerar_codigo_if_else_da_arvore(
    tree,
    ohe_feature_names,
    label_encoder,
    nome_funcao="decidir_efeito_one_hot"
)

# Nome do arquivo .py que vamos criar
nome_arquivo = "decidir_efeito_one_hot.py"

# Escreve o c√≥digo no arquivo
with open(nome_arquivo, "w", encoding="utf-8") as f:
    f.write("# Arquivo gerado automaticamente a partir da √°rvore de decis√£o\n")
    f.write("# Fun√ß√£o decidir_efeito_one_hot baseada em features one-hot\n\n")
    f.write(codigo_if_else)
    f.write("\n")  # quebra de linha final

# Faz o download do arquivo para sua m√°quina
files.download(nome_arquivo)

"""Esta c√©lula d√° continuidade ao desenvolvimento do modelo de Machine Learning, focando em duas abordagens: √Årvore de Decis√£o e Random Forest. Ap√≥s instalar as bibliotecas e carregar o arquivo Excel (similar ao passo anterior), ele realiza o pr√©-processamento dos dados, combinando os efeitos e codificando as caracter√≠sticas. Em seguida, treina uma √Årvore de Decis√£o (`DecisionTreeClassifier`) e uma Floresta Aleat√≥ria (`RandomForestClassifier`) para comparar a acur√°cia. A matriz de confus√£o da Random Forest √© exibida para avaliar visualmente o desempenho. Finalmente, uma √Årvore de Decis√£o √© treinada com 100% dos dados para gerar um conjunto completo de regras e uma fun√ß√£o Python de infer√™ncia para prever efeitos com base nas pr√©-condi√ß√µes."""

# ============================================
# 1. Instalar depend√™ncias
# ============================================
!pip install -q pandas scikit-learn openpyxl matplotlib

# ============================================
# 2. Imports
# ============================================
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier, export_text, _tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

import matplotlib.pyplot as plt
from google.colab import files


# ============================================
# 3. Upload do arquivo Excel
# ============================================
print("Fa√ßa upload do arquivo .xlsx com as tabelas:")
uploaded = files.upload()

excel_filename = list(uploaded.keys())[0]
print(f"Arquivo carregado: {excel_filename}")


# ============================================
# 4. Ler TODAS as abas de tabela e unificar
# ============================================
xlsx = pd.ExcelFile(excel_filename)

all_dfs = []
for sheet_name in xlsx.sheet_names:
    # Se quiser filtrar s√≥ abas que come√ßam com "Tabela", descomente:
    # if not sheet_name.lower().startswith("tabela"):
    #     continue
    df = pd.read_excel(xlsx, sheet_name=sheet_name)
    df["Tabela_ID"] = sheet_name
    all_dfs.append(df)

full_df = pd.concat(all_dfs, ignore_index=True)
print("Formato do dataset unificado:", full_df.shape)
print("Colunas encontradas:")
print(full_df.columns)


# ============================================
# 5. Detectar pr√©-condi√ß√µes e combinar Effects*
# ============================================
precondition_cols = [c for c in full_df.columns if "preconditions" in c.lower()]
effect_source_cols = [c for c in full_df.columns if c.lower().startswith("effects")]

print("\nColunas de pr√©-condi√ß√µes:", precondition_cols)
print("Colunas de efeitos brutas:", effect_source_cols)

if len(precondition_cols) == 0 or len(effect_source_cols) == 0:
    raise ValueError("N√£o foi poss√≠vel detectar colunas de pr√©-condi√ß√£o e/ou efeito. Verifique os nomes.")

# juntar todas as colunas Effects* em uma coluna √∫nica Effect_Final
effects_df = full_df[effect_source_cols].copy()
effects_df = effects_df.replace("-", np.nan)
effects_df = effects_df.replace("", np.nan)

for c in effects_df.columns:
    effects_df[c] = effects_df[c].astype("string")

def combine_effects(row):
    vals = [v for v in row if pd.notna(v)]
    if not vals:
        return np.nan
    return "+".join(vals)

full_df["Effect_Final"] = effects_df.apply(combine_effects, axis=1)

print("\nExemplos de Effect_Final:")
print(full_df[["Effect_Final"]].head(15))


# ============================================
# 6. Montar X e y
# ============================================
feature_cols = precondition_cols + ["Tabela_ID"]
effect_col = "Effect_Final"

data = full_df[feature_cols + [effect_col]].copy()
data = data.dropna(subset=[effect_col])  # remove linhas sem efeito

# preencher NaN nas features com "MISSING"
for col in feature_cols:
    data[col] = data[col].astype("string").fillna("MISSING")

X = data[feature_cols]
y = data[effect_col].astype("string")

print("\nTamanho do dataset final:", data.shape)
print("\nDistribui√ß√£o das classes (efeitos):")
print(y.value_counts())


# ============================================
# 7. Pr√©-processamento e divis√£o treino/teste
# ============================================
categorical_features = feature_cols

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_encoded,
    test_size=0.2,
    random_state=42,
    shuffle=True  # sem stratify por causa das muitas classes raras
)

print("\nTamanhos:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)


# ============================================
# 8. MODELO 1 ‚Äì Decision Tree (para regras/c√≥digo)
# ============================================
clf_dt = DecisionTreeClassifier(
    random_state=42,
    max_depth=None,
    min_samples_leaf=1
)

model_dt = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_dt)
])

model_dt.fit(X_train, y_train)
y_pred_dt = model_dt.predict(X_test)
acc_dt = accuracy_score(y_test, y_pred_dt)
print(f"\nAcur√°cia Decision Tree no conjunto de teste: {acc_dt:.4f}")


# ============================================
# 9. MODELO 2 ‚Äì Random Forest (para compara√ß√£o)
# ============================================
clf_rf = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    max_depth=None,
    min_samples_leaf=1,
    n_jobs=-1
)

model_rf = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_rf)
])

model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"Acur√°cia Random Forest no conjunto de teste: {acc_rf:.4f}")


# ============================================
# 10. Matriz de confus√£o (usando Random Forest)
# ============================================
cm = confusion_matrix(y_test, y_pred_rf)

labels_used = np.unique(np.concatenate([y_test, y_pred_rf]))
display_labels = label_encoder.classes_[labels_used]

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=display_labels
)

fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(ax=ax, xticks_rotation="vertical", cmap="viridis", colorbar=True)
plt.title("Matriz de confus√£o - Efeitos (Effect_Final) - Random Forest")
plt.tight_layout()
plt.show()


# ============================================
# 11. Treinar √Årvore em 100% dos dados para gerar regras/c√≥digo
# ============================================
# Aqui usamos TODAS as amostras para aprender as regras da especifica√ß√£o
clf_dt_full = DecisionTreeClassifier(
    random_state=42,
    max_depth=None,
    min_samples_leaf=1
)

model_dt_full = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_dt_full)
])

model_dt_full.fit(X, y_encoded)  # X e y_encoded completos

# extrair objetos internos
preprocessor.fit(X)
ohe = preprocessor.named_transformers_["cat"]
ohe_feature_names = ohe.get_feature_names_out(categorical_features)

tree = model_dt_full.named_steps["clf"]

tree_rules_text = export_text(tree, feature_names=list(ohe_feature_names))
print("\n===== REGRAS DA √ÅRVORE DE DECIS√ÉO (TREINADA EM 100% DOS DADOS) =====")
print(tree_rules_text[:2000], "...\n")  # imprime s√≥ o come√ßo pra n√£o explodir o output


# ============================================
# 12. Fun√ß√£o de infer√™ncia em alto n√≠vel (usa pipeline + encoder)
# ============================================
def prever_efeito(preconditions_dict):
    """
    preconditions_dict: dicion√°rio com as colunas originais de X:
      - todas as 'Preconditions*'
      - 'Tabela_ID'
    Valores faltantes ser√£o preenchidos com 'MISSING'.
    """
    row = {}
    for col in feature_cols:
        val = preconditions_dict.get(col, "MISSING")
        row[col] = "MISSING" if (val is None or str(val) == "nan") else str(val)

    row_df = pd.DataFrame([row])
    y_pred_enc = model_dt_full.predict(row_df)[0]
    efeito = label_encoder.inverse_transform([y_pred_enc])[0]
    return efeito

print("\n===== EXEMPLO DE PREDI√á√ÉO (usando √°rvore completa) =====")
exemplo = {}
for col in feature_cols:
    exemplo[col] = X.iloc[0][col]
print("Entrada:", exemplo)
print("Efeito previsto:", prever_efeito(exemplo))


# ============================================
# 13. Gerar c√≥digo Python if/else a partir da √°rvore
# ============================================
def gerar_codigo_if_else_da_arvore(tree, feature_names, label_encoder, nome_funcao="decidir_efeito_one_hot"):
    """
    Gera c√≥digo Python com if/else em cima das features j√° one-hot-encoded (bin√°rias).
    """
    tree_ = tree.tree_
    classes = label_encoder.classes_

    def recurse(node, depth):
        indent = "    " * depth

        # condi√ß√£o correta para n√≥ folha
        if tree_.feature[node] == _tree.TREE_UNDEFINED:
            class_id = np.argmax(tree_.value[node][0])
            class_label = classes[class_id]
            return f"{indent}return '{class_label}'\n"
        else:
            feature = feature_names[tree_.feature[node]]
            threshold = tree_.threshold[node]
            cond = f"{feature} <= {threshold:.5f}"

            code = ""
            code += f"{indent}if {cond}:\n"
            code += recurse(tree_.children_left[node], depth + 1)
            code += f"{indent}else:\n"
            code += recurse(tree_.children_right[node], depth + 1)
            return code

    header = f"def {nome_funcao}({', '.join(feature_names)}):\n"
    body = recurse(0, 1)
    return header + body

codigo_if_else = gerar_codigo_if_else_da_arvore(
    tree,
    ohe_feature_names,
    label_encoder,
    nome_funcao="decidir_efeito_one_hot"
)

print("\n===== IN√çCIO DO C√ìDIGO GERADO (if/else) =====")
print(codigo_if_else[:2000], "...\n")  # s√≥ o come√ßo


# ============================================
# 14. Salvar o c√≥digo gerado em um arquivo .py e baixar
# ============================================
nome_arquivo = "decidir_efeito_one_hot.py"

with open(nome_arquivo, "w", encoding="utf-8") as f:
    f.write("# Arquivo gerado automaticamente a partir da √°rvore de decis√£o\n")
    f.write("# Fun√ß√£o decidir_efeito_one_hot baseada em features one-hot da especifica√ß√£o\n\n")
    f.write(codigo_if_else)
    f.write("\n")

print(f"Arquivo {nome_arquivo} gerado.")
files.download(nome_arquivo)

"""Esta c√©lula finaliza o processo de Machine Learning, concentrando-se na gera√ß√£o de c√≥digo C++ a partir do modelo de √Årvore de Decis√£o. Ap√≥s instalar as depend√™ncias e carregar o arquivo Excel, o c√≥digo realiza as mesmas etapas de pr√©-processamento e treinamento de modelos (√Årvore de Decis√£o e Random Forest) para garantir a base de dados e o modelo treinados. A principal fun√ß√£o deste bloco √© a `gerar_codigo_if_else_da_arvore_cpp`, que converte a l√≥gica da √°rvore de decis√£o em uma s√©rie de declara√ß√µes `if/else` em C++. Essa fun√ß√£o gera um arquivo `.cpp` que pode ser usado para integrar as regras de decis√£o em sistemas C++ existentes. O arquivo C++ gerado √© ent√£o automaticamente baixado."""

# ============================================
# 1. Instalar depend√™ncias
# ============================================
!pip install -q pandas scikit-learn openpyxl matplotlib

# ============================================
# 2. Imports
# ============================================
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier, export_text, _tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

import matplotlib.pyplot as plt
from google.colab import files

# ============================================
# 3. Upload do arquivo Excel
# ============================================
print("Fa√ßa upload do arquivo .xlsx com as tabelas:")
uploaded = files.upload()

excel_filename = list(uploaded.keys())[0]
print(f"Arquivo carregado: {excel_filename}")

# ============================================
# 4. Ler TODAS as abas de tabela e unificar
# ============================================
xlsx = pd.ExcelFile(excel_filename)

all_dfs = []
for sheet_name in xlsx.sheet_names:
    # Se quiser filtrar s√≥ abas que come√ßam com "Tabela", descomente:
    # if not sheet_name.lower().startswith("tabela"):
    #     continue
    df = pd.read_excel(xlsx, sheet_name=sheet_name)
    df["Tabela_ID"] = sheet_name
    all_dfs.append(df)

full_df = pd.concat(all_dfs, ignore_index=True)
print("Formato do dataset unificado:", full_df.shape)
print("Colunas encontradas:")
print(full_df.columns)

# ============================================
# 5. Detectar pr√©-condi√ß√µes e combinar Effects*
# ============================================
precondition_cols = [c for c in full_df.columns if "preconditions" in c.lower()]
effect_source_cols = [c for c in full_df.columns if c.lower().startswith("effects")]

print("\nColunas de pr√©-condi√ß√µes:", precondition_cols)
print("Colunas de efeitos brutas:", effect_source_cols)

if len(precondition_cols) == 0 or len(effect_source_cols) == 0:
    raise ValueError("N√£o foi poss√≠vel detectar colunas de pr√©-condi√ß√£o e/ou efeito. Verifique os nomes.")

# juntar todas as colunas Effects* em uma coluna √∫nica Effect_Final
effects_df = full_df[effect_source_cols].copy()
effects_df = effects_df.replace("-", np.nan)
effects_df = effects_df.replace("", np.nan)

for c in effects_df.columns:
    effects_df[c] = effects_df[c].astype("string")

def combine_effects(row):
    vals = [v for v in row if pd.notna(v)]
    if not vals:
        return np.nan
    return "+".join(vals)

full_df["Effect_Final"] = effects_df.apply(combine_effects, axis=1)

print("\nExemplos de Effect_Final:")
print(full_df[["Effect_Final"]].head(15))

# ============================================
# 6. Montar X e y
# ============================================
feature_cols = precondition_cols + ["Tabela_ID"]
effect_col = "Effect_Final"

data = full_df[feature_cols + [effect_col]].copy()
data = data.dropna(subset=[effect_col])  # remove linhas sem efeito

# preencher NaN nas features com "MISSING"
for col in feature_cols:
    data[col] = data[col].astype("string").fillna("MISSING")

X = data[feature_cols]
y = data[effect_col].astype("string")

print("\nTamanho do dataset final:", data.shape)
print("\nDistribui√ß√£o das classes (efeitos):")
print(y.value_counts())

# ============================================
# 7. Pr√©-processamento e divis√£o treino/teste
# ============================================
categorical_features = feature_cols

preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_encoded,
    test_size=0.2,
    random_state=42,
    shuffle=True  # sem stratify por causa das muitas classes raras
)

print("\nTamanhos:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)

# ============================================
# 8. MODELO 1 ‚Äì Decision Tree (para regras/c√≥digo)
# ============================================
clf_dt = DecisionTreeClassifier(
    random_state=42,
    max_depth=None,
    min_samples_leaf=1
)

model_dt = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_dt)
])

model_dt.fit(X_train, y_train)
y_pred_dt = model_dt.predict(X_test)
acc_dt = accuracy_score(y_test, y_pred_dt)
print(f"\nAcur√°cia Decision Tree no conjunto de teste: {acc_dt:.4f}")

# ============================================
# 9. MODELO 2 ‚Äì Random Forest (opcional, s√≥ pra compara√ß√£o)
# ============================================
clf_rf = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    max_depth=None,
    min_samples_leaf=1,
    n_jobs=-1
)

model_rf = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_rf)
])

model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"Acur√°cia Random Forest no conjunto de teste: {acc_rf:.4f}")

# Matriz de confus√£o do RF
cm = confusion_matrix(y_test, y_pred_rf)
labels_used = np.unique(np.concatenate([y_test, y_pred_rf]))
display_labels = label_encoder.classes_[labels_used]

disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=display_labels
)

fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(ax=ax, xticks_rotation="vertical", cmap="viridis", colorbar=True)
plt.title("Matriz de confus√£o - Efeitos (Effect_Final) - Random Forest")
plt.tight_layout()
plt.show()

# ============================================
# 10. Treinar √Årvore em 100% dos dados (para gerar c√≥digo)
# ============================================
clf_dt_full = DecisionTreeClassifier(
    random_state=42,
    max_depth=None,
    min_samples_leaf=1
)

model_dt_full = Pipeline(steps=[
    ("preprocess", preprocessor),
    ("clf", clf_dt_full)
])

model_dt_full.fit(X, y_encoded)  # usa todos os dados

# Ajustar preprocessor em X completo para ter feature_names corretos
preprocessor.fit(X)
ohe = preprocessor.named_transformers_["cat"]
ohe_feature_names = ohe.get_feature_names_out(categorical_features)

tree = model_dt_full.named_steps["clf"]

# (Opcional) ver regras em texto
tree_rules_text = export_text(tree, feature_names=list(ohe_feature_names))
print("\n===== IN√çCIO DAS REGRAS DA √ÅRVORE (texto) =====")
print(tree_rules_text[:1500], "...\n")

# ============================================
# 11. Fun√ß√£o de infer√™ncia em alto n√≠vel (Python, usando pipeline)
# ============================================
def prever_efeito(preconditions_dict):
    """
    preconditions_dict: dicion√°rio com as colunas originais de X:
      - todas as 'Preconditions*'
      - 'Tabela_ID'
    """
    row = {}
    for col in feature_cols:
        val = preconditions_dict.get(col, "MISSING")
        row[col] = "MISSING" if (val is None or str(val) == "nan") else str(val)

    row_df = pd.DataFrame([row])
    y_pred_enc = model_dt_full.predict(row_df)[0]
    efeito = label_encoder.inverse_transform([y_pred_enc])[0]
    return efeito

print("\n===== EXEMPLO DE PREDI√á√ÉO (√°rvore completa) =====")
exemplo = {col: X.iloc[0][col] for col in feature_cols}
print("Entrada:", exemplo)
print("Efeito previsto:", prever_efeito(exemplo))

# ============================================
# 12. Gerar C√ìDIGO C++ (if/else) a partir da √°rvore
# ============================================
def gerar_codigo_if_else_da_arvore_cpp(tree, feature_names, label_encoder,
                                       nome_funcao="decidir_efeito_one_hot"):
    """
    Gera c√≥digo C++ com if/else em cima das features one-hot.
    Cada feature ser√° um par√¢metro do tipo double (0.0 ou 1.0).
    """
    tree_ = tree.tree_
    classes = label_encoder.classes_

    def recurse(node, depth):
        indent = "    " * depth
        if tree_.feature[node] == _tree.TREE_UNDEFINED:
            # n√≥ folha
            class_id = np.argmax(tree_.value[node][0])
            class_label = classes[class_id]
            return f'{indent}return "{class_label}";\n'
        else:
            feature = feature_names[tree_.feature[node]]
            threshold = tree_.threshold[node]
            cond = f"{feature} <= {threshold:.5f}"

            code = ""
            code += f"{indent}if ({cond}) {{\n"
            code += recurse(tree_.children_left[node], depth + 1)
            code += f"{indent}}} else {{\n"
            code += recurse(tree_.children_right[node], depth + 1)
            code += f"{indent}}}\n"
            return code

    # Montar assinatura da fun√ß√£o em C++
    params = ",\n    ".join([f"double {fn}" for fn in feature_names])

    header = '#include <string>\n\n'
    header += f'std::string {nome_funcao}(\n    {params}\n) {{\n'
    body = recurse(0, 1)
    footer = "}\n"

    return header + body + footer

codigo_cpp = gerar_codigo_if_else_da_arvore_cpp(
    tree,
    ohe_feature_names,
    label_encoder,
    nome_funcao="decidir_efeito_one_hot"
)

print("\n===== IN√çCIO DO C√ìDIGO C++ GERADO =====")
print(codigo_cpp[:2000], "...\n")

# ============================================
# 13. Salvar o c√≥digo C++ em um arquivo .cpp e baixar
# ============================================
nome_arquivo_cpp = "decidir_efeito_one_hot.cpp"

with open(nome_arquivo_cpp, "w", encoding="utf-8") as f:
    f.write("// Arquivo gerado automaticamente a partir da √°rvore de decis√£o\n")
    f.write("// Fun√ß√£o decidir_efeito_one_hot baseada nas features one-hot\n\n")
    f.write(codigo_cpp)
    f.write("\n")

print(f"Arquivo {nome_arquivo_cpp} gerado.")
files.download(nome_arquivo_cpp)